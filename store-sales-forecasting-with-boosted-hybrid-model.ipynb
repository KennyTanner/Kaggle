{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kennytanner/store-sales-forecasting-with-boosted-hybrid-model?scriptVersionId=120663817\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### Please take a look at my Store Sales Forecasting EDA Notebook if you haven't yet at\nhttps://www.kaggle.com/kennytanner/store-sales-forecasting-extensive-eda/","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nimport pickle\n\nimport warnings\nfrom IPython import get_ipython\nget_ipython().config.InlineBackend.figure_format = 'retina'\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:24.916377Z","iopub.execute_input":"2023-02-28T16:22:24.916849Z","iopub.status.idle":"2023-02-28T16:22:24.938135Z","shell.execute_reply.started":"2023-02-28T16:22:24.916808Z","shell.execute_reply":"2023-02-28T16:22:24.936713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Contents\n## 1. Big Decisions\n#### 1.1. Model Picks and Sacrificing Data\n#### 1.2. Imputting Zeros\n## 2. Hybrid Class\n## 3. Feature Engineering\n#### 3.1. Dates\n#### 3.2. X1 Features\n#### 3.3. X2 Features\n#### 3.4. Checking the Feature Set\n## 4. Validation & Error Analysis\n## 5. Predictions\n## 6. Results","metadata":{}},{"cell_type":"code","source":"df_sales= pd.read_csv(\n    '/kaggle/input/store-sales-time-series-forecasting/train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True)\n\ndf_test = pd.read_csv(\n    '/kaggle/input/store-sales-time-series-forecasting/test.csv',\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True)\n\ndf_stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\ndf_transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv', parse_dates=['date'])\ndf_oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv', parse_dates=['date'])\n\ndf_holidays_events = pd.read_csv(\n    \"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\",\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',},\n    parse_dates=['date'],\n    infer_datetime_format=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:24.941332Z","iopub.execute_input":"2023-02-28T16:22:24.941863Z","iopub.status.idle":"2023-02-28T16:22:29.136741Z","shell.execute_reply.started":"2023-02-28T16:22:24.94181Z","shell.execute_reply":"2023-02-28T16:22:29.135554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Big Decisions\n## 1.1. Model Picks & Sacrificing Data\nThese required some thought and testing. So define here to avoid careless mistakes:","metadata":{}},{"cell_type":"code","source":"# time range to train before train test split\nfull_train_start_day = datetime.datetime(2015, 6, 16) \nfull_train_end_day = datetime.datetime(2017, 8, 15)\n\n# time range for train validation split \ntrain_start_day = full_train_start_day\ntrain_end_day = datetime.datetime(2017, 7, 30)\nval_start_day = datetime.datetime(2017, 7, 31)\nval_end_day = datetime.datetime(2017, 8, 15)\n\n# time range of test set\ntest_start_day = datetime.datetime(2017, 8, 16)\ntest_end_day = datetime.datetime(2017, 8, 31)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.137961Z","iopub.execute_input":"2023-02-28T16:22:29.138399Z","iopub.status.idle":"2023-02-28T16:22:29.145505Z","shell.execute_reply.started":"2023-02-28T16:22:29.138357Z","shell.execute_reply":"2023-02-28T16:22:29.144281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data pre June 2015 is very patchy and better results are found by removing it completely.","metadata":{}},{"cell_type":"markdown","source":"There was an Eartherquake in Ecuador over this period, killing nearly 700 people, and injuring 16,000. https://en.wikipedia.org/wiki/2016_Ecuador_earthquake\n\n- It destroyed Manta's central shopping district. With store 53, open in Manta at the time, being impacted and store 52 opening locally in April 2017 also.\n- This was only 170 km (110 mi) from the capital Quito, where it was also felt strongly and 18 of the stores are located.\n\n**Please see my exploratory data analysis notebook for store locations and detailed EDA**\n(https://www.kaggle.com/kennytanner/store-sales-forecasting-extensive-eda/)","metadata":{}},{"cell_type":"markdown","source":"Selecting the number of days for the second model to ignore! (max_lag)\n- Introducing lags and rolling statistics will create NaNs and inacurate statistics. This occurs on the edges of the data as the information required to produce them falls outside the data available.\n\n**Importantly:**\n- The first model will be a feature-transforming algorithm. Its main purpose is to find any trend and extrapolate. (Target-transforming models cannot do this.)\n- Then second model will be target-transforming and will be fit on the residuals of the first model. This model is intended to be good at capturing the interactions.\n- By producing the lag features here we can just tell this model to ignore the effected rows, without loosing data at every forecasting step.","metadata":{}},{"cell_type":"code","source":"max_lag = 7","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.146845Z","iopub.execute_input":"2023-02-28T16:22:29.147163Z","iopub.status.idle":"2023-02-28T16:22:29.16419Z","shell.execute_reply.started":"2023-02-28T16:22:29.147132Z","shell.execute_reply":"2023-02-28T16:22:29.162837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forecasting Day by Day and then Refitting All Days\n- To forecast 1 day at a time (so that lag features can be utilised)\n- Then each new forecasted day is then factored in. Such that the other forecasted days are then reforecast using the larger data set","metadata":{}},{"cell_type":"code","source":"mod_1 = LinearRegression()\nmod_2 = XGBRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.168136Z","iopub.execute_input":"2023-02-28T16:22:29.169023Z","iopub.status.idle":"2023-02-28T16:22:29.175247Z","shell.execute_reply.started":"2023-02-28T16:22:29.168968Z","shell.execute_reply":"2023-02-28T16:22:29.173702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- I trialed a few different models. Lasso performed better than ridge regression. Perhaps some of the features were more noise than aid (lasso can remove their contribution by sending the coefficients to zero). I did orginally think ridge would perform better, especially when I could add in, and play around with feature engingeering with the second model, so I thought this was interesting.\n\n- In the end LinearRegression performed the best. Linear regression does not impose any constraints on the model parameters. So can fit more complex models without any regularization. Lasso imposes a penalty on large coefficients, which can restrict the model's ability to fit complex models.\n\n- With the trends removed, a nice and powerful gradient decent model is in order to fit to the residuals. And XGBoost performed well as the second model.","metadata":{}},{"cell_type":"markdown","source":"## 1.2. Imputting Zeros","metadata":{}},{"cell_type":"code","source":"Not_sold = df_sales.groupby([\"store_nbr\", \"family\"]).sum().reset_index().sort_values([\"family\",\"store_nbr\"])\nNot_sold = Not_sold[Not_sold.sales == 0]\nNot_sold.store_nbr = Not_sold.store_nbr.astype('str')\nNot_sold","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:22:29.17691Z","iopub.execute_input":"2023-02-28T16:22:29.178035Z","iopub.status.idle":"2023-02-28T16:22:29.43452Z","shell.execute_reply.started":"2023-02-28T16:22:29.177946Z","shell.execute_reply":"2023-02-28T16:22:29.433191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While I do appreciate that this is leakage into the validation set, aren't we trying to predict the test data not the validation data in the end.\n\nIf that is the goal, and we have no data but zeros to train on for these families at these stores. In my opinion, our best bet is to see if there is any indication that this will change over the period we are forecasting. And if not, then to predict zeros!\n\nSo here I check if there are any 'department opening' sales, to indicate that this is the case.","metadata":{}},{"cell_type":"code","source":"Test_promos = df_test.groupby([\"store_nbr\", \"family\"]).sum().reset_index().sort_values([\"family\", \"store_nbr\"])\nTest_promos = Test_promos[Test_promos.onpromotion>0]\n\nTest_promos = Test_promos.set_index([\"store_nbr\", \"family\"]).index\n\nSet_to_check = set(Not_sold.set_index([\"store_nbr\", \"family\"]).index)\nStarted_promotion_in_test_period = [x for x in Set_to_check if x in Test_promos]\n\nStarted_promotion_in_test_period","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:22:29.436515Z","iopub.execute_input":"2023-02-28T16:22:29.43733Z","iopub.status.idle":"2023-02-28T16:22:29.467697Z","shell.execute_reply.started":"2023-02-28T16:22:29.437278Z","shell.execute_reply":"2023-02-28T16:22:29.466287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like Lawn and Garden Departments have opened up in stores 14, 30 and 54, and we have an interesting situation at play!\n\nFortunately the model will still learn from all the other data we have and will predict some sales. Probably not the best predictions, and I would really love to hear if anyone knows some good tactics! Please post in the comments if so!\n\nPerhaps identifying items that went from no sales ever, to sales, in a feature (called say \"first_15_days_open\") and identifying Lawn and Garden in stores 14, 30 & 54 as those over our test data date range. That would be interesting. I may still come back and do this (in which case you probably won't be reading this). But it would certainly be unwise to impute zeros here.\n\nOn the otherhand, those stores that have never sold books, baby care or ladieswear, surely they would at least have an opening sale on? If I spoke better Spanish this might be easy to research but for now I will impute zeros for those that have zero sales thus far and also no upcoming promotions.","metadata":{}},{"cell_type":"code","source":"zeros_to_impute = list(Not_sold.set_index([\"store_nbr\", \"family\"]).index.drop(Started_promotion_in_test_period).values)\n#zeros_to_impute","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.469458Z","iopub.execute_input":"2023-02-28T16:22:29.470566Z","iopub.status.idle":"2023-02-28T16:22:29.481508Z","shell.execute_reply.started":"2023-02-28T16:22:29.470524Z","shell.execute_reply":"2023-02-28T16:22:29.480193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Hybrid Class","metadata":{}},{"cell_type":"code","source":"class Hybrid():\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None\n        self.stack_cols = None\n        self.y_resid = None\n\n    def fit1(self, X_1, y, stack_cols=None):\n        self.model_1.fit(X_1, y) # train 1st model\n        y_fit = pd.DataFrame(\n            self.model_1.predict(X_1), # predict from 1st model\n            index=X_1.index,\n            columns=y.columns,\n        )\n        self.y_resid = y - y_fit # residuals from 1st model\n        self.y_resid = self.y_resid.stack(stack_cols).squeeze()  # wide to long\n        \n    def fit2(self, X_2, first_n_rows_to_ignore, stack_cols=None):\n        self.model_2.fit(X_2.iloc[first_n_rows_to_ignore*1782: , :], self.y_resid.iloc[first_n_rows_to_ignore*1782:]) # Train 2nd model\n        self.y_columns = y.columns # Save for predict method\n        self.stack_cols = stack_cols # Save for predict method\n\n    def predict(self, X_1, X_2, first_n_rows_to_ignore):\n        y_pred = pd.DataFrame(\n            self.model_1.predict(X_1.iloc[first_n_rows_to_ignore: , :]), # predictions from 1st model\n            index=X_1.iloc[first_n_rows_to_ignore: , :].index,\n            columns=self.y_columns,\n        )\n        y_pred = y_pred.stack(self.stack_cols).squeeze()  # wide to long\n        y_pred += self.model_2.predict(X_2.iloc[first_n_rows_to_ignore*1782: , :]) # Add model_2 predictions to model_1 predictions\n        return y_pred.unstack(self.stack_cols)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.484109Z","iopub.execute_input":"2023-02-28T16:22:29.485112Z","iopub.status.idle":"2023-02-28T16:22:29.506824Z","shell.execute_reply.started":"2023-02-28T16:22:29.485069Z","shell.execute_reply":"2023-02-28T16:22:29.505542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering\nThe final selection of features shown here result from:\n- EDA (https://www.kaggle.com/kennytanner/store-sales-forecasting-extensive-eda/)\n- Validation experiments with combinaions that appeared promising\n- Error analysis on the validation set\n- Care not to overtrain to the test set (making selections based on validation - to relfect a real production and deployment scenario)","metadata":{}},{"cell_type":"markdown","source":"I classify stores open before the data as mature stores, as this matches the stores dataframe classification.\n\nDistribution of sales over time, for stores are commonly seen to have features associated with being new in comparison to having \"matured\". (A spike in interest on opening and then often a lower sales rate than those who have developed a regular custmer base.)\nTherefore it may help to differentiate those stores that have passed these initial stages from the newbies on the block, and including their opening dates.","metadata":{}},{"cell_type":"code","source":"daily_sales_sum = (\n    df_sales.drop(columns = 'onpromotion')\n    .groupby(['date', 'store_nbr'])\n    .sum()\n    .reset_index())\n\nmature_stores = daily_sales_sum[daily_sales_sum['date'] == '2013-01-02'].store_nbr.values","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.509128Z","iopub.execute_input":"2023-02-28T16:22:29.510026Z","iopub.status.idle":"2023-02-28T16:22:29.730662Z","shell.execute_reply.started":"2023-02-28T16:22:29.509972Z","shell.execute_reply":"2023-02-28T16:22:29.729736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also played around with using an opening date instead. I believe this might be better for newly opening stores but saw no improvement in validation and had no reason for concern in the 15 day test data period.\n\nThis might be something to look into futher though. Especially in implementation over the long term.","metadata":{}},{"cell_type":"code","source":"stores_of_type = {}\nfor store_type in df_stores.type.unique():\n    stores_of_type[store_type] = list(map(str, df_stores[df_stores.type == store_type].store_nbr))\n    print(f'store type {store_type} = {stores_of_type[store_type]}')","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.732375Z","iopub.execute_input":"2023-02-28T16:22:29.733079Z","iopub.status.idle":"2023-02-28T16:22:29.745947Z","shell.execute_reply.started":"2023-02-28T16:22:29.733031Z","shell.execute_reply":"2023-02-28T16:22:29.744436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_store_types(df=df_sales):\n    for store_type in df_stores.type.unique():\n        df[f'store_type_{store_type}'] = df.store_nbr.isin(stores_of_type[store_type])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:29.748119Z","iopub.execute_input":"2023-02-28T16:22:29.748821Z","iopub.status.idle":"2023-02-28T16:22:29.756477Z","shell.execute_reply.started":"2023-02-28T16:22:29.74877Z","shell.execute_reply":"2023-02-28T16:22:29.754968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sales['date'] = df_sales.date.dt.to_period('D')\n\n# And to get the missing Christmas days, in the multi index form\nmultiindex = pd.MultiIndex.from_product([df_sales[\"store_nbr\"].unique(),\n                                      df_sales[\"family\"].unique(),\n                                      pd.date_range(start=\"2013-1-1\", end=\"2017-8-15\", freq=\"D\").to_period('D')]\n                                     ,names=[\"store_nbr\",\"family\", \"date\"])\ndf_sales= df_sales.set_index([\"store_nbr\",\"family\", \"date\"]).reindex(multiindex, fill_value=0).sort_index()\n\n\ndf_sales= df_sales.unstack(['store_nbr', 'family'])\ndf_sales= df_sales.stack(['store_nbr', 'family'])\n\ndf_test['date'] = df_test.date.dt.to_period('D')\ndf_test = df_test.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:22:29.758512Z","iopub.execute_input":"2023-02-28T16:22:29.759044Z","iopub.status.idle":"2023-02-28T16:22:54.347101Z","shell.execute_reply.started":"2023-02-28T16:22:29.758993Z","shell.execute_reply":"2023-02-28T16:22:54.345679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_oil['date'] = df_oil.date.dt.to_period('D')\ndf_oil = df_oil.set_index('date')\n\n# Linear Interpolation\ndf_oil = df_oil.dcoilwtico.resample('D').sum().reset_index()\n\ndf_oil['dcoilwtico'] = np.where(df_oil['dcoilwtico'] == 0, np.nan, df_oil['dcoilwtico'])\ndf_oil['dcoilwtico_interpolated'] = df_oil.dcoilwtico.interpolate()\n\ndf_oil = df_oil.set_index('date')\ndf_oil.dcoilwtico_interpolated = df_oil.dcoilwtico_interpolated.fillna(method='bfill')","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.352781Z","iopub.execute_input":"2023-02-28T16:22:54.353181Z","iopub.status.idle":"2023-02-28T16:22:54.39049Z","shell.execute_reply.started":"2023-02-28T16:22:54.353136Z","shell.execute_reply":"2023-02-28T16:22:54.389169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def threshold_for_expensive(threshold=60):\n    df_oil[\"expensive_oil\"]=df_oil.dcoilwtico_interpolated >= threshold\n    return df_oil.drop(columns=['dcoilwtico', 'dcoilwtico_interpolated'])","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.392599Z","iopub.execute_input":"2023-02-28T16:22:54.393068Z","iopub.status.idle":"2023-02-28T16:22:54.399724Z","shell.execute_reply.started":"2023-02-28T16:22:54.39302Z","shell.execute_reply":"2023-02-28T16:22:54.398437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expensive_oil_add(df, threshold=60):\n    df = df.join(threshold_for_expensive(threshold))\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.401372Z","iopub.execute_input":"2023-02-28T16:22:54.402431Z","iopub.status.idle":"2023-02-28T16:22:54.412535Z","shell.execute_reply.started":"2023-02-28T16:22:54.402382Z","shell.execute_reply":"2023-02-28T16:22:54.411278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Dates\nCorrecting the Good Friday error immediately","metadata":{}},{"cell_type":"code","source":"df_holidays_events['date'] = df_holidays_events['date'].replace({'2013-04-29':pd.to_datetime('2013-03-29')})\n# Quite an easy data collection error to check and correct - Good Friday was March 29th in 2013 (in Columbia too)\ndf_holidays_events = df_holidays_events.set_index('date').to_period('D').sort_index()","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.414141Z","iopub.execute_input":"2023-02-28T16:22:54.414484Z","iopub.status.idle":"2023-02-28T16:22:54.429252Z","shell.execute_reply.started":"2023-02-28T16:22:54.414453Z","shell.execute_reply":"2023-02-28T16:22:54.428304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without the Fourier features (seasonal oscillations), one could simply remove the missing Christmas days....","metadata":{}},{"cell_type":"code","source":"# missing_Xmas_days = list((pd.date_range(start=\"2013-01-01\", end=\"2017-08-15\").difference(df_sales.date)))\n\n# for _, day in enumerate(missing_Xmas_days):\n#     missing_Xmas_days[_] = day.to_period(freq='D')\n\n# missing_Xmas_days\n\n# calendar = calendar.drop(missing_Xmas_days, axis=0)\n\n# df_sales = df_sales.set_index([\"date\", \"store_nbr\", \"family\"])\n# df_sales","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:22:54.43109Z","iopub.execute_input":"2023-02-28T16:22:54.431469Z","iopub.status.idle":"2023-02-28T16:22:54.444444Z","shell.execute_reply.started":"2023-02-28T16:22:54.431432Z","shell.execute_reply":"2023-02-28T16:22:54.443461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While there are also smaller holidays, the national holidays effect all stores and are by far the largest proportion to effect any store, by a massive margin so lets start by identifying the dates and effects of those.","metadata":{}},{"cell_type":"code","source":"calendar = pd.DataFrame(index=pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\ncalendar['dofw'] = calendar.index.dayofweek\n\ncalendar['wd'] = True\ncalendar.loc[calendar.dofw > 4, 'wd'] = False","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:22:54.445738Z","iopub.execute_input":"2023-02-28T16:22:54.446712Z","iopub.status.idle":"2023-02-28T16:22:54.463042Z","shell.execute_reply.started":"2023-02-28T16:22:54.446671Z","shell.execute_reply":"2023-02-28T16:22:54.461793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"National_Holidays = df_holidays_events[df_holidays_events.locale == 'National'].copy()\n\n# Having more than one holiday on the same day messes with the shape of the datafrae by adding row\n# The model should do perfectly fine with only one holiday identified for the date (so just using the first)\nNational_Holidays = National_Holidays.groupby(National_Holidays.index).first()\n\ncalendar = calendar.merge(National_Holidays, how='left', left_index=True, right_index=True)\n\ncalendar.loc[calendar.type == 'Bridge'  , 'wd'] = False\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = True\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = False\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True ), 'wd'] = True\n\ncalendar","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.464785Z","iopub.execute_input":"2023-02-28T16:22:54.465192Z","iopub.status.idle":"2023-02-28T16:22:54.545895Z","shell.execute_reply.started":"2023-02-28T16:22:54.465153Z","shell.execute_reply":"2023-02-28T16:22:54.544541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wd = calendar\ndf_wd[\"date\"] = df_wd.index\ndf_wd = df_wd.set_index(\"date\")\ndf_wd = df_wd.drop(columns=['dofw', 'type', 'locale', 'locale_name', 'description', 'transferred'])\n\ndef wd_add(df, start_date, end_date):  # for long format\n    df = df.join(df_wd.loc[start_date:end_date])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.547515Z","iopub.execute_input":"2023-02-28T16:22:54.548275Z","iopub.status.idle":"2023-02-28T16:22:54.559893Z","shell.execute_reply.started":"2023-02-28T16:22:54.548197Z","shell.execute_reply":"2023-02-28T16:22:54.558304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. X1 Features","metadata":{}},{"cell_type":"code","source":"def make_DeterministicProcess_features(df):\n    y = df.loc[:, 'sales']\n    fourier_m = CalendarFourier(freq='M', order=4) # monthly showed good results\n    dp = DeterministicProcess(                     # weekly and annual performed badly on validation set\n        index=y.index,\n        constant=True,\n        order=1,\n        seasonal=True,\n        additional_terms=[fourier_m],\n        drop=True,\n    )\n    return y, dp","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.562702Z","iopub.execute_input":"2023-02-28T16:22:54.563274Z","iopub.status.idle":"2023-02-28T16:22:54.572317Z","shell.execute_reply.started":"2023-02-28T16:22:54.563194Z","shell.execute_reply":"2023-02-28T16:22:54.570783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_X1_features(df, start_date, end_date, is_test_set=False):\n    if is_test_set:\n        X1 = df.rename_axis('date')\n    else:\n        y, dp = make_DeterministicProcess_features(df)\n        X1 = dp.in_sample()\n\n    # All closed Chirstmas days so far.\n    # And all the stores were closed on new year except 25 and 36 sometimes\n    # But in order to make the fourier features it was important to include these days\n    # Therefore it is important to identify them!\n    \n    X1['NewYear'] = (X1.index.dayofyear == 1)   \n    X1['Christmas'] = (X1.index=='2016-12-25') | (X1.index=='2015-12-25') | (X1.index=='2014-12-25') | (X1.index=='2013-12-25')\n    \n    X1['wd']   = calendar.loc[start_date:end_date]['wd'].values    \n    X1['type'] = calendar.loc[start_date:end_date]['type'].values\n    X1['description'] = calendar.loc[start_date:end_date]['description'].values\n    X1 = pd.get_dummies(X1, columns=['type', 'description'], drop_first=False)\n    X1.drop(['type_Work Day', 'type_Event', 'type_Holiday'], axis=1, inplace=True)\n    \n    if is_test_set:\n        return X1\n    else:\n        return X1, y, dp","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.574186Z","iopub.execute_input":"2023-02-28T16:22:54.574783Z","iopub.status.idle":"2023-02-28T16:22:54.589837Z","shell.execute_reply.started":"2023-02-28T16:22:54.574707Z","shell.execute_reply":"2023-02-28T16:22:54.588467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. X2 Features","metadata":{}},{"cell_type":"markdown","source":"I classify stores open before the data as mature stores, as this matches the stores dataframe classification.\n\nDistribution of sales over time, for stores are commonly seen to have features associated with being new in comparison to having \"matured\". (A spike in interest on opening and then often a lower sales rate than those who have developed a regular custmer base.)\nTherefore it may help to differentiate those stores that have passed these initial stages from the newbies on the block, and including their opening dates.","metadata":{}},{"cell_type":"code","source":"def encode_categoricals(df, columns):\n    le = LabelEncoder()\n    for col in columns:\n        df[col] = le.fit_transform(df[col])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.591061Z","iopub.execute_input":"2023-02-28T16:22:54.591479Z","iopub.status.idle":"2023-02-28T16:22:54.603067Z","shell.execute_reply.started":"2023-02-28T16:22:54.591441Z","shell.execute_reply":"2023-02-28T16:22:54.601976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_X2_lags(ts, lags, lead_time=1, name='y', stack_cols=None):\n    ts = ts.unstack(stack_cols)\n    df = pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i, freq=\"D\")\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n    df = df.stack(stack_cols).reset_index()\n    df = encode_categoricals(df, stack_cols)\n    df = df.set_index('date').sort_values(by=stack_cols)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.604709Z","iopub.execute_input":"2023-02-28T16:22:54.605132Z","iopub.status.idle":"2023-02-28T16:22:54.616367Z","shell.execute_reply.started":"2023-02-28T16:22:54.605088Z","shell.execute_reply":"2023-02-28T16:22:54.615318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_X2_features(df, y_resid, start_date, end_date):\n    stack_columns = ['store_nbr', 'family']\n    \n    # promo_lag features\n    promo_target_df = make_X2_lags(df['onpromotion'].squeeze(), lags=max_lag, name='promo', stack_cols=['store_nbr', 'family'])\n    \n    promo_target_df['promo_mean_rolling_7'] = promo_target_df['promo_lag_1'].rolling(window=7, center=False).mean()\n    promo_target_df['promo_median_rolling_70'] = promo_target_df['promo_lag_1'].rolling(window=70, center=False).median().fillna(method='bfill')\n    promo_target_df['promo_median_rolling_140'] = promo_target_df['promo_lag_1'].rolling(window=140, center=False).median().fillna(method='bfill')\n    # for larger rolling window medians, backfilling seems reasonable as medians shouldn't change too much.\n    \n    # y_lag features\n    sales_target_df = make_X2_lags(y_resid, lags=max_lag, name='y_res', stack_cols=stack_columns)\n    \n    sales_target_df['y_mean_rolling_7'] = sales_target_df['y_res_lag_1'].rolling(window=7, center=False).mean()\n    sales_target_df['y_median_rolling_14'] = sales_target_df['y_res_lag_1'].rolling(window=14, center=False).median().fillna(method='bfill')\n    sales_target_df['y_median_rolling_70'] = sales_target_df['y_res_lag_1'].rolling(window=70, center=False).median().fillna(method='bfill')\n    sales_target_df['y_median_rolling_140'] = sales_target_df['y_res_lag_1'].rolling(window=140, center=False).median().fillna(method='bfill')\n    \n    sales_target_df['y_last_half_year_max']= sales_target_df['y_res_lag_1'].rolling(window=182, center=False).max().fillna(method='bfill')\n    sales_target_df['y_last_half_year_min']= sales_target_df['y_res_lag_1'].rolling(window=182, center=False).min().fillna(method='bfill')\n    sales_target_df['y_last_month_std']= sales_target_df['y_res_lag_1'].rolling(window=29, center=False).std().fillna(method='bfill')\n    \n    # interactions and other features\n    sales_target_df['lag1-2_week_median_sales'] = sales_target_df['y_res_lag_1'] - sales_target_df['y_median_rolling_14']\n    sales_target_df = sales_target_df.drop(columns='y_median_rolling_14')\n    \n    df = df.reset_index(stack_columns)\n    X2 = encode_categoricals(df, stack_columns)\n    \n    X2['NewYear'] = (X2.index.dayofyear == 1)\n    X2['Christmas'] = (X2.index=='2016-12-25') | (X2.index=='2015-12-25') | (X2.index=='2014-12-25') | (X2.index=='2013-12-25')\n    \n    # All public sector wages in Ecuador are paid on the 15th and the last day of the month\n    X2['wage_day'] = (X2.index.day == X2.index.daysinmonth) | (X2.index.day == 15)\n    X2['wage_day_lag_1'] = (X2.index.day == 1) | (X2.index.day == 16)\n    #X2['wage_day_lag_2'] = (X2.index.day == 2) | (X2.index.day == 17) # another surprise that even just a second lag was too many!\n    \n    X2['store_mature'] = X2['store_nbr'].isin(list(map(str, mature_stores)))\n    \n    X2 = expensive_oil_add(X2, threshold=60)\n    X2 = add_store_types(X2)\n    X2 = wd_add(X2, start_date, end_date)\n    \n    X2['promo_mean'] = X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001\n    X2['promo_ratio'] = X2.onpromotion / (X2.groupby(['store_nbr', 'family'])['onpromotion'].transform(\"mean\") + 0.000001)\n    \n    X2 = X2.merge(sales_target_df, on=['date', 'store_nbr', 'family'], how='left')\n    X2 = X2.merge(promo_target_df, on=['date', 'store_nbr', 'family'], how='left')\n    return X2","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.618543Z","iopub.execute_input":"2023-02-28T16:22:54.618906Z","iopub.status.idle":"2023-02-28T16:22:54.638836Z","shell.execute_reply.started":"2023-02-28T16:22:54.618871Z","shell.execute_reply":"2023-02-28T16:22:54.637717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4. Checking the Feature Set\nThis is a simple and much faster look into the outcomes. More for debugging than for experimentation (no one day at a time and refitting). The validation and error analysis (section 4) provide better feed back on how well the features perform.","metadata":{}},{"cell_type":"code","source":"%%time\ndf_sales_in_date_range = df_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\n\nmodel = Hybrid(model_1=mod_1, model_2=mod_2)\n\nX_1, y, dp = make_X1_features(df_sales_in_date_range, full_train_start_day, full_train_end_day) \nmodel.fit1(X_1, y, stack_cols=['store_nbr', 'family'])\nX_2 = make_X2_features(df_sales_in_date_range\n                       .drop('sales', axis=1)\n                       .stack(['store_nbr', 'family']),\n                       model.y_resid, full_train_start_day, full_train_end_day)\nmodel.fit2(X_2, max_lag, stack_cols=['store_nbr', 'family'])\n\ny_pred = model.predict(X_1, X_2, max_lag).clip(0.0)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:22:54.640376Z","iopub.execute_input":"2023-02-28T16:22:54.640787Z","iopub.status.idle":"2023-02-28T16:29:01.377682Z","shell.execute_reply.started":"2023-02-28T16:22:54.640749Z","shell.execute_reply":"2023-02-28T16:29:01.376287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncateFloat(data):\n    return tuple( [\"{0:.2f}\".format(x) if isinstance(x,float) else (x if not isinstance(x,tuple) else truncateFloat(x)) for x in data])\n\ntemp = X_2[(X_2.store_nbr == 1) & (X_2.family == 3)]\ntemp.iloc[max_lag: , :].apply(lambda s: truncateFloat(s))\n\ntemp.apply(lambda s: truncateFloat(s)).head(10) # fit method will skip over the nan rows","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:29:01.380037Z","iopub.execute_input":"2023-02-28T16:29:01.380557Z","iopub.status.idle":"2023-02-28T16:29:01.716491Z","shell.execute_reply.started":"2023-02-28T16:29:01.380502Z","shell.execute_reply":"2023-02-28T16:29:01.715628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_1.iloc[max_lag: , :].apply(lambda s: truncateFloat(s))","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:29:01.71764Z","iopub.execute_input":"2023-02-28T16:29:01.718368Z","iopub.status.idle":"2023-02-28T16:29:01.82115Z","shell.execute_reply.started":"2023-02-28T16:29:01.718315Z","shell.execute_reply":"2023-02-28T16:29:01.819787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look at how the model fits the training data","metadata":{}},{"cell_type":"code","source":"#display(df_sales.index.get_level_values('family').unique())\nSTORE_NBR = '1'  # 1-54\nFAMILY = 'BEVERAGES' \n\nax = y.loc(axis=1)[STORE_NBR, FAMILY].plot(figsize=(16, 4), color='silver')\nax = y_pred.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax,  color='cornflowerblue')\nplt.xlabel(\"Sales\")\nax.set_title(f'{FAMILY.title()} Sales at Store {STORE_NBR}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T16:29:01.822857Z","iopub.execute_input":"2023-02-28T16:29:01.824204Z","iopub.status.idle":"2023-02-28T16:29:02.319893Z","shell.execute_reply.started":"2023-02-28T16:29:01.824149Z","shell.execute_reply":"2023-02-28T16:29:02.318621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Validation & Some Error Analysis","metadata":{}},{"cell_type":"code","source":"training_days = (train_end_day - train_start_day).days + 1\nvalidation_days = (val_end_day - val_start_day).days + 1\nprint(\"training data set (excluding validation days) has\", training_days, \"days\")\nprint(\"validation data set has\", validation_days, \"days\\n\")\nprint(\"validating a hybrid with the first model being: \\n \\n \", mod_1)\nprint('\\n and the second being: \\n \\n ', mod_2)\nprint('\\n \\n with max_lag=', max_lag, 'days')\n\ndf_sales_in_date_range = df_sales.unstack(['store_nbr', 'family']).loc[train_start_day:train_end_day]\nsales_in_val_range = df_sales.unstack(['store_nbr', 'family']).loc[val_start_day:val_end_day]\ny_val = y[val_start_day:val_end_day]\n\nmodel_for_val = Hybrid(model_1=mod_1, model_2=mod_2)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:29:02.321622Z","iopub.execute_input":"2023-02-28T16:29:02.322086Z","iopub.status.idle":"2023-02-28T16:29:03.643083Z","shell.execute_reply.started":"2023-02-28T16:29:02.322047Z","shell.execute_reply":"2023-02-28T16:29:03.641739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#initial fit on train portion of train/val split\nX_1_train, y_train, dp_val = make_X1_features(df_sales_in_date_range, train_start_day, train_end_day) \nmodel_for_val.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family'])\nX_2_train = make_X2_features(df_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_val.y_resid, full_train_start_day, full_train_end_day)\nmodel_for_val.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\ny_fit = model_for_val.predict(X_1_train, X_2_train, max_lag).clip(0.0)\n\n# loop through forecast, one day at a time\ndp_for_full_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\nfor step in range(validation_days):\n        dp_steps_so_far = dp_for_full_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([df_sales_in_date_range,\n                                       sales_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n        X_1_val = make_X1_features(X_1_combined_dp_data, train_start_day, val_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_val = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_val.y_resid, train_start_day, val_start_day+pd.Timedelta(days=step))\n\n        y_pred_combined = model_for_val.predict(X_1_val, X_2_val, max_lag).clip(0.0)\n\n        y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]]) \n        model_for_val.fit1(X_1_val, y_plus_y_val, stack_cols=['store_nbr', 'family']) \n        model_for_val.fit2(X_2_val, max_lag, stack_cols=['store_nbr', 'family'])\n\n        rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5\n        print(f'Validation RMSLE: {rmsle_valid:.5f}', \"for\", val_start_day+pd.Timedelta(days=step)) # To see progress","metadata":{"execution":{"iopub.status.busy":"2023-02-28T16:29:03.64484Z","iopub.execute_input":"2023-02-28T16:29:03.645256Z","iopub.status.idle":"2023-02-28T18:09:54.525126Z","shell.execute_reply.started":"2023-02-28T16:29:03.645201Z","shell.execute_reply":"2023-02-28T18:09:54.523489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred_combined[val_start_day:val_end_day]\n\nfor zeros in zeros_to_impute:\n    y_pred[zeros]=0.0\n\nprint(\"\\ny_pred: \")\ndisplay(y_pred.apply(lambda s: truncateFloat(s)))\n    \nif type(model_for_val.model_2) == XGBRegressor:\n        pickle.dump(model_for_val.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T18:09:54.526775Z","iopub.execute_input":"2023-02-28T18:09:54.527121Z","iopub.status.idle":"2023-02-28T18:09:54.865568Z","shell.execute_reply.started":"2023-02-28T18:09:54.527087Z","shell.execute_reply":"2023-02-28T18:09:54.864348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSLE is calculated as:\n$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\nwhere:\n\n\n n is the total number of instances,\n\n$\\hat{y}_i$ is the predicted value of the target for instance (i),\n\n$y_i$ is the actual value of the target for instance (i), and,\n\nlog is the natural logarithm.","metadata":{}},{"cell_type":"code","source":"rmsle_train = mean_squared_log_error(y_train.iloc[max_lag: , :].clip(0.0), y_fit) ** 0.5\nrmsle_valid = mean_squared_log_error(y_val.clip(0.0), y_pred) ** 0.5\nprint()\nprint(f'Training RMSLE: {rmsle_train:.5f}')\nprint(f'Validation RMSLE: {rmsle_valid:.5f}')\n    \ny_predict = y_pred.stack(['store_nbr', 'family']).reset_index()\ny_target = y_val.stack(['store_nbr', 'family']).reset_index().copy()\ny_target.rename(columns={y_target.columns[3]:'sales'}, inplace=True)\ny_target['sales_pred'] = y_predict[0].clip(0.0) # Sales should be >= 0\ny_target['store_nbr'] = y_target['store_nbr'].astype(int)\n\nprint('\\nValidation RMSLE by family')\ndisplay(y_target.groupby('family').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])).sort_values(ascending=False))\n\nprint('\\nValidation RMSLE by store')\ndisplay(y_target.sort_values(by=\"store_nbr\").groupby('store_nbr').apply(lambda r: mean_squared_log_error(r['sales'], r['sales_pred'])).sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2023-02-28T18:09:54.867342Z","iopub.execute_input":"2023-02-28T18:09:54.867732Z","iopub.status.idle":"2023-02-28T18:09:55.145471Z","shell.execute_reply.started":"2023-02-28T18:09:54.867694Z","shell.execute_reply":"2023-02-28T18:09:55.144044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_target","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T18:09:55.147296Z","iopub.execute_input":"2023-02-28T18:09:55.147685Z","iopub.status.idle":"2023-02-28T18:09:55.168683Z","shell.execute_reply.started":"2023-02-28T18:09:55.147648Z","shell.execute_reply":"2023-02-28T18:09:55.167364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def error_analysis(FAMILY=None, STORE_NBR= None):\n    y=y_target.set_index('date')\n    if FAMILY:\n        if FAMILY in y_target.family.unique():\n            y=y[y.family == FAMILY]\n            if not STORE_NBR:\n                y.groupby('date').sales.mean().plot(color='silver', label='mean sales')\n                y.groupby('date').sales_pred.mean().plot(color='cornflowerblue', label='mean prediction')\n                plt.legend()\n                plt.ylabel('Mean Sales')\n                plt.xlabel('Date')\n                plt.title(f'All Stores Comparison for {FAMILY.title()}')\n                plt.show()\n                for STORE_NBR in range(1,55):\n                    plt.clf()\n                    y.sales[y.store_nbr == STORE_NBR].plot(color='black', label='sales')\n                    y.sales_pred[y.store_nbr == STORE_NBR].plot(color='fuchsia', label='prediction')\n                    plt.legend()\n                    plt.ylabel('Sales')\n                    plt.xlabel('Date')\n                    plt.title(f'Store #{STORE_NBR} {FAMILY.title()}')\n                    plt.show()\n                return\n        else:\n            return (f\"departments (families) available are {y_target.family.unique()}\")\n    if STORE_NBR:\n        if STORE_NBR <= 54:\n            if FAMILY:\n                y.sales[y.store_nbr == STORE_NBR].plot(color='black', label='sales')\n                y.sales_pred[y.store_nbr == STORE_NBR].plot(color='fuchsia', label='prediction')\n                plt.ylabel('Sales')\n                plt.xlabel('Date')\n                plt.legend()\n                plt.title(f\"Store {STORE_NBR} {FAMILY.title()} Comparison\")\n                plt.show()\n            else:\n                y=y[y.store_nbr == STORE_NBR]\n                y.groupby('date').sales.mean().plot(color='silver', label='mean sales')\n                y.groupby('date').sales_pred.mean().plot(color='tomato', label='mean prediction')\n                plt.legend()\n                plt.ylabel('Mean Sales')\n                plt.xlabel('Date')\n                plt.title(f'All Departments (Families) Comparison for Store #{STORE_NBR}')\n                plt.show()\n                for FAMILY in y_target.family.unique():\n                    plt.clf()\n                    y.sales[y.family == FAMILY].plot(color='black', label='sales')\n                    y.sales_pred[y.family == FAMILY].plot(color='fuchsia', label='prediction')\n                    plt.ylabel('Sales')\n                    plt.xlabel('Date')\n                    plt.legend()\n                    plt.title(f'Store #{STORE_NBR} {FAMILY.title()}')\n                    plt.show()\n        else:\n            return \"max number of stores=54\"\n    else:\n        for STORE_NBR in range(1,55):\n            y1=y[y.store_nbr == STORE_NBR].groupby(['date']).mean()\n            y1.groupby('date').sales.mean().plot(color='silver', label='mean sales')\n            y1.groupby('date').sales_pred.mean().plot(color='cornflowerblue', label='mean prediction')\n            plt.legend()\n            plt.ylabel('Mean Sales')\n            plt.xlabel('Date')\n            plt.title(f'Average Sales Across all Departments (Families) for Store # {STORE_NBR}')\n            plt.show()\n            plt.clf()\n        for FAMILY in y_target.family.unique():\n            y1=y[y.family == FAMILY].groupby(['date']).mean()\n            y1.groupby('date').sales.mean().plot(color='silver', label='mean sales')\n            y1.groupby('date').sales_pred.mean().plot(color='tomato', label='mean prediction')\n            plt.legend()\n            plt.ylabel('Mean Sales')\n            plt.xlabel('Date')\n            plt.title(f'Average Sales Across all Stores for {FAMILY.title()}')\n            plt.show()\n            plt.clf()","metadata":{"execution":{"iopub.status.busy":"2023-02-28T18:09:55.170745Z","iopub.execute_input":"2023-02-28T18:09:55.171173Z","iopub.status.idle":"2023-02-28T18:09:55.193807Z","shell.execute_reply.started":"2023-02-28T18:09:55.171118Z","shell.execute_reply":"2023-02-28T18:09:55.192316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_analysis(FAMILY=\"SCHOOL AND OFFICE SUPPLIES\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T18:09:55.195247Z","iopub.execute_input":"2023-02-28T18:09:55.195901Z","iopub.status.idle":"2023-02-28T18:10:10.890042Z","shell.execute_reply.started":"2023-02-28T18:09:55.195845Z","shell.execute_reply":"2023-02-28T18:10:10.888748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To capture the back to school season perhaps one hot encoding the month of the year (or even week of the year if you aren't overly concerned with massively increasing the dimentionality) would be a significant help. Label encoding isn't really appropriate as yearly seasons like this are unlikely to be linear! I did play around with label enoding over the week and month thinking their might be some relationships in there after wage day or weekend excitement. But took them out, finding the working day and wage day related features to be better.\n\nMy EDA (https://www.kaggle.com/kennytanner/store-sales-forecasting-extensive-eda/) did not show significant seasonality in the School and Office Supplies anyway, and I expect the first model captured most already, so I didn't worry about it in the end. \n\nBut if you did go ahead and try that (or have other interesteing suggestions) that please let me know in the comments!","metadata":{}},{"cell_type":"markdown","source":"# 5. Predictions","metadata":{}},{"cell_type":"code","source":"train_days = (full_train_end_day - full_train_start_day).days + 1\ntest_days = (test_end_day - test_start_day).days + 1\n\nprint(\"data trained over\", train_days, \"days\")\nprint(\"test forecasting period is\", test_days, \"days through\", test_end_day, \"\\n\")\nprint(\"validating a hybrid with the first model being: \\n \\n \", mod_1)\nprint('\\n and the second being: \\n \\n ', mod_2)\nprint('\\n \\n with max_lag=', max_lag, ' days')\ndf_sales_in_date_range = df_sales.unstack(['store_nbr', 'family']).loc[full_train_start_day:full_train_end_day]\nsales_in_test = df_test.unstack(['store_nbr', 'family']).drop('id', axis=1)\n\nmodel_for_test = Hybrid(model_1=mod_1, model_2=mod_2)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T18:10:10.891944Z","iopub.execute_input":"2023-02-28T18:10:10.893098Z","iopub.status.idle":"2023-02-28T18:10:11.553624Z","shell.execute_reply.started":"2023-02-28T18:10:10.893044Z","shell.execute_reply":"2023-02-28T18:10:11.552341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#initial fit on train portion of train/test split\nX_1_train, y_train, dp_test = make_X1_features(df_sales_in_date_range, full_train_start_day, full_train_end_day)\nmodel_for_test.fit1(X_1_train, y_train, stack_cols=['store_nbr', 'family']) \nX_2_train = make_X2_features(df_sales_in_date_range\n                           .drop('sales', axis=1)\n                           .stack(['store_nbr', 'family']),\n                           model_for_test.y_resid, full_train_start_day, full_train_end_day)\nmodel_for_test.fit2(X_2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\ndp_for_full_X1_test_date_range = dp_test.out_of_sample(steps=test_days)\nfor step in range(test_days):\n        dp_steps_so_far = dp_for_full_X1_test_date_range.loc[test_start_day:test_start_day+pd.Timedelta(days=step),:]\n\n        X_1_combined_dp_data = pd.concat([dp_test.in_sample(), dp_steps_so_far])\n        X_2_combined_data = pd.concat([df_sales_in_date_range,\n                                       sales_in_test.loc[test_start_day:test_start_day+pd.Timedelta(days=step), :]])\n        X_1_test = make_X1_features(X_1_combined_dp_data, train_start_day, test_start_day+pd.Timedelta(days=step), is_test_set=True)\n        X_2_test = make_X2_features(X_2_combined_data\n                                    .drop('sales', axis=1)\n                                    .stack(['store_nbr', 'family']),\n                                    model_for_test.y_resid, train_start_day, test_start_day+pd.Timedelta(days=step))\n\n        y_forecast_combined = model_for_test.predict(X_1_test, X_2_test, max_lag).clip(0.0)\n\n        y_plus_y_test = pd.concat([y_train, y_forecast_combined.iloc[-(step+1):]])\n        model_for_test.fit1(X_1_test, y_plus_y_test, stack_cols=['store_nbr', 'family'])\n        model_for_test.fit2(X_2_test, max_lag, stack_cols=['store_nbr', 'family'])\n        print(\"finished forecast for\", test_start_day+pd.Timedelta(days=step)) # To see progress\n        \ndisplay(y_forecast_combined[test_start_day:test_end_day])","metadata":{"execution":{"iopub.status.busy":"2023-02-28T18:10:11.555735Z","iopub.execute_input":"2023-02-28T18:10:11.55629Z","iopub.status.idle":"2023-02-28T19:53:40.575451Z","shell.execute_reply.started":"2023-02-28T18:10:11.556232Z","shell.execute_reply":"2023-02-28T19:53:40.574005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_forecast = pd.DataFrame(y_forecast_combined.loc[test_start_day:test_end_day].clip(0.0))\n    \nif type(model_for_test.model_2) == XGBRegressor:\n        pickle.dump(model_for_test.model_2, open(\"xgb_temp.pkl\", \"wb\"))\n        m2 = pickle.load(open(\"xgb_temp.pkl\", \"rb\"))\n        print(\"XGBRegressor paramaters:\\n\",m2.get_xgb_params(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-02-28T19:53:40.577841Z","iopub.execute_input":"2023-02-28T19:53:40.578346Z","iopub.status.idle":"2023-02-28T19:53:40.608201Z","shell.execute_reply.started":"2023-02-28T19:53:40.57829Z","shell.execute_reply":"2023-02-28T19:53:40.60707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for zeros in zeros_to_impute:\n    y_forecast[zeros]=0.0\n    y_fit[zeros]=0.0","metadata":{"execution":{"iopub.status.busy":"2023-02-28T19:53:40.615431Z","iopub.execute_input":"2023-02-28T19:53:40.616254Z","iopub.status.idle":"2023-02-28T19:53:40.656324Z","shell.execute_reply.started":"2023-02-28T19:53:40.616189Z","shell.execute_reply":"2023-02-28T19:53:40.654775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Results","metadata":{}},{"cell_type":"code","source":"# display(df_sales.index.get_level_values('family').unique())\nSTORE_NBR = '1'  # 1 - 54\nFAMILY = 'PRODUCE'\n\nax = y.loc(axis=1)[STORE_NBR, FAMILY].plot(figsize=(16, 4), color='dimgrey')\nax = y_pred.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, color='greenyellow', marker='.')\nax = y_forecast.loc(axis=1)[STORE_NBR, FAMILY].plot(ax=ax, color='blueviolet', marker='.')\nax.set_title(f'{FAMILY.title()} Sales at Store {STORE_NBR}')\nax.set_ylabel(f'{FAMILY.title()} Sales')\n\n# fit = black\n# validation prediction (before model was retrained for final predictions) = green\n# forecast = purple","metadata":{"execution":{"iopub.status.busy":"2023-02-28T19:53:40.658288Z","iopub.execute_input":"2023-02-28T19:53:40.658679Z","iopub.status.idle":"2023-02-28T19:53:41.020852Z","shell.execute_reply.started":"2023-02-28T19:53:40.65864Z","shell.execute_reply":"2023-02-28T19:53:41.019592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FAMILIES = 33   # 1 - 33\nSTORE_NBR = '1'    # 1 - 54\n\ny_for_store = y.loc(axis=1)[STORE_NBR]\ny_fit_for_store = y_fit.loc(axis=1)[STORE_NBR]\ny_pred_for_store = y_pred.loc(axis=1)[STORE_NBR]\ny_forecast_for_store = y_forecast.loc(axis=1)[STORE_NBR]\nfamilies = y_for_store.columns[0:NUM_FAMILIES]\n\naxs = y_for_store.loc(axis=1)[families].plot(\n    subplots=True, sharex=True, figsize=(16, 3*NUM_FAMILIES), alpha=0.6,\n)\n_ = y_fit_for_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='dimgrey', ax=axs)\n_ = y_pred_for_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='greenyellow', ax=axs, marker='.')\n_ = y_forecast_for_store.loc(axis=1)[families].plot(subplots=True, sharex=True, color='slateblue', ax=axs, marker='.')\n\nfor ax, family in zip(axs, families):\n    ax.legend([])\n    ax.set_ylabel(f'{family.title()} Sales')\n    ax.set_title(f'{family.title()} Sales at Store {STORE_NBR}')\n    ax.tick_params(labelbottom=True)\n\nplt.tight_layout()\nplt.show()\n    \n# fit = black\n# validation prediction (before model was retrained for final predictions) = green\n# forecast = purple","metadata":{"execution":{"iopub.status.busy":"2023-02-28T19:53:41.022804Z","iopub.execute_input":"2023-02-28T19:53:41.023323Z","iopub.status.idle":"2023-02-28T19:53:56.452501Z","shell.execute_reply.started":"2023-02-28T19:53:41.023266Z","shell.execute_reply":"2023-02-28T19:53:56.45079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = y_forecast.stack(['store_nbr', 'family'])\nsubmission = pd.DataFrame(submission, columns=['sales'])\nsubmission = submission.join(df_test.id).reindex(columns=['id', 'sales'])\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-28T19:53:56.455678Z","iopub.execute_input":"2023-02-28T19:53:56.456441Z","iopub.status.idle":"2023-02-28T19:53:56.726894Z","shell.execute_reply.started":"2023-02-28T19:53:56.456392Z","shell.execute_reply":"2023-02-28T19:53:56.725403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As always, thank you to the community. This has been a fun learning experiance. Please see below my main sources. They have been a massive help to me and I would recommend these to everyone in their own analysis journey with this dataset.\n\nHoang Pham Viet: Past Time Series competitions _ Winning solutions (https://www.kaggle.com/competitions/store-sales-time-series-forecasting/discussion/278372)\n\nRyan Holbrook: Kaggle Official Time Series Course (https://www.kaggle.com/learn/time-series)\n\nFilterJoe: Time Series Bonus Lesson (Unofficial) (https://www.kaggle.com/code/filterjoe/time-series-bonus-lesson-unofficial/notebook)\n\nEkrem Bayar: Store Sales TS Forecasting - A Comprehensive Guide (https://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide/notebook)\n\nJavier Ruiz: Oil Prices Matter More Than We Think(https://www.kaggle.com/competitions/store-sales-time-series-forecasting/discussion/298626)\n\nPratik Narkhede: What is onpromotion, type, cluster?(https://www.kaggle.com/competitions/store-sales-time-series-forecasting/discussion/289764)\n\nThe Devastator: Store Sales - Top Discussion Posts (https://www.kaggle.com/competitions/store-sales-time-series-forecasting/discussion/332439)","metadata":{}},{"cell_type":"markdown","source":"And just in case you missed the other links ;) My exploratory data analysis notebook is https://www.kaggle.com/kennytanner/store-sales-forecasting-extensive-eda/","metadata":{}}]}